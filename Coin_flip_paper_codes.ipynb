{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calipark1213/3340FinalProject/blob/main/Coin_flip_paper_codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "import pandas as pd\n",
        "import statistics as st\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input, LeakyReLU, BatchNormalization\n",
        "import numpy as np\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVC, OneClassSVM\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_score, recall_score, classification_report, roc_auc_score, average_precision_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# set seed\n",
        "random.seed(10)\n",
        "np.random.seed(10)\n"
      ],
      "metadata": {
        "id": "phxxVXw9iNvg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cuIxPO8NNXU"
      },
      "outputs": [],
      "source": [
        "#@title Sequence truncation\n",
        "sequences = []\n",
        "with open('fake.txt', 'r') as file: # fake.txt is a txt file of all the coin flip sequences that were handwritten\n",
        "  for line in file:\n",
        "    if line[0] == '0' or line[0] == '1':\n",
        "      line = line.strip()\n",
        "      newline = [int(line[i]) for i in range(200)]\n",
        "      sequences.append(newline)\n",
        "\n",
        "print(len(sequences))\n",
        "\n",
        "data = pd.DataFrame(np.array(sequences))\n",
        "\n",
        "data.to_csv('fake_sequences_handwritten.csv', float_format='{:f}'.format, encoding='utf-8', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tricky algorithm (from Mike's paper)"
      ],
      "metadata": {
        "id": "RJ2psrR4i7Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bernoulli:\n",
        "    @staticmethod\n",
        "    def pmf(x, p):\n",
        "        f = p ** x * (1 - p) ** (1 - x)\n",
        "        return f\n",
        "\n",
        "    @staticmethod\n",
        "    def mean(p):\n",
        "        return p\n",
        "\n",
        "    @staticmethod\n",
        "    def var(p):\n",
        "        return p * (1 - p)\n",
        "\n",
        "    @staticmethod\n",
        "    def std(p):\n",
        "        return Bernoulli.var(p) ** (1 / 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rvs(p, size):\n",
        "        rvs = np.random.choice([-1, 1], size=size, p=[1 - p, p])\n",
        "        return rvs\n",
        "\n",
        "\n",
        "def compute_prob(p, k, sequence, probabilities, i):\n",
        "    if k == 0:\n",
        "        return p\n",
        "\n",
        "    m_values = sequence[i - k:i - 1]\n",
        "    beta_values = [probabilities[j] for j in range(len(m_values))]\n",
        "    sum_val = np.sum([m * beta for m, beta in zip(m_values, beta_values)])\n",
        "    denom = 2 ** (k - 1) * Bernoulli.pmf(1, probabilities[i - k])\n",
        "    prob = p + sum_val / denom\n",
        "\n",
        "    # Ensure the conditional probability is within [0, 1]\n",
        "    prob = max(0, min(1, prob))\n",
        "    return prob\n",
        "\n",
        "\n",
        "def generate_sequence(N, k):\n",
        "    valid_params = False\n",
        "    max_attempts = 1000\n",
        "    attempt_count = 0\n",
        "    sequence = []\n",
        "    probabilities = []\n",
        "    while not valid_params and attempt_count < max_attempts:\n",
        "        sequence = []\n",
        "        probabilities = []\n",
        "\n",
        "        for _ in range(N):\n",
        "            p = random.uniform(0, 1)\n",
        "            probabilities.append(p)\n",
        "            if len(sequence) == 0:\n",
        "                sequence.append(1 if random.uniform(0, 1) <= p else -1)\n",
        "            else:\n",
        "                probability = compute_prob(p, min(k, len(sequence)), sequence,\n",
        "                                           probabilities, len(sequence))\n",
        "                while not 0 <= probability <= 1:\n",
        "                    p = random.uniform(0, 1)\n",
        "                    probabilities[-1] = p\n",
        "                    probability = compute_prob(p, min(k, len(sequence)),\n",
        "                                               sequence, probabilities,\n",
        "                                               len(sequence))\n",
        "\n",
        "                    # Regenerate beta values\n",
        "                    beta_values = []\n",
        "                    for j in range(1, min(k, len(sequence)) + 1):\n",
        "                        beta = random.uniform(-1, 1)\n",
        "                        beta_values.append(beta)\n",
        "                    probabilities[-k:] = beta_values\n",
        "\n",
        "                sequence.append(1 if random.uniform(0, 1) <= probability else -1)\n",
        "\n",
        "        sequence = [0 if val == -1 else val for val in sequence]\n",
        "\n",
        "        p_bar, beta_bar = estimate_params(sequence, probabilities, Nc)\n",
        "        error = calculate_error(probabilities[-1], beta_bar, p_bar, beta_bar, Nc)\n",
        "        if error <= 0.5:  # Adjusted threshold for easier generation\n",
        "            valid_params = True\n",
        "\n",
        "        attempt_count += 1\n",
        "\n",
        "    if not valid_params:\n",
        "        print(f\"Warning: Maximum attempts \"\n",
        "              f\"({max_attempts}) reached without finding valid parameters.\")\n",
        "        sequence = []\n",
        "        probabilities = []\n",
        "\n",
        "    return sequence, probabilities\n",
        "\n",
        "\n",
        "def estimate_params(sequence, probabilities, Nc):\n",
        "    p_bar = np.mean(sequence)\n",
        "    beta_bar = []\n",
        "\n",
        "    for j in range(1, Nc + 1):\n",
        "        beta = np.mean([sequence[i] * sequence[i - j] for i in range(j, len(sequence))])\n",
        "        beta_bar.append(beta)\n",
        "\n",
        "    return p_bar, beta_bar\n",
        "\n",
        "\n",
        "def calculate_error(p, beta, p_bar, beta_bar, Nc):\n",
        "    error = (p - p_bar) ** 2\n",
        "    for j in range(Nc):\n",
        "        error += (beta[j] - beta_bar[j]) ** 2\n",
        "    error /= (Nc + 1)\n",
        "    return error\n",
        "\n",
        "# Parameters\n",
        "Nf = 200  # Number of flips\n",
        "Nc = 199  # Number of pair-wise covariances\n",
        "\n",
        "# Generate multiple sequences\n",
        "num_sequences = 137\n",
        "all_sequences = []\n",
        "all_probabilities = []\n",
        "\n",
        "for _ in range(num_sequences):\n",
        "    sequence, probabilities = generate_sequence(Nf, Nc)\n",
        "    all_sequences.append(sequence)\n",
        "    all_probabilities.append(probabilities)\n",
        "\n",
        "# Print generated sequences\n",
        "print(\"Generated Sequences:\")\n",
        "for i, sequence in enumerate(all_sequences):\n",
        "    print(sequence)\n",
        "\n",
        "# Exporting sequences\n",
        "csv_filename = \"fake_sequences_tricky.csv\"\n",
        "\n",
        "# Writing the nested list to the CSV file\n",
        "with open(csv_filename, mode='w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    for row in all_sequences:\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(f\"Nested list has been written to '{csv_filename}'\")\n"
      ],
      "metadata": {
        "id": "_GiqaeouNm21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN"
      ],
      "metadata": {
        "id": "OWkSzxIYi2_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the binary sequences\n",
        "seq_length = 200\n",
        "latent_dim = 100  # Dimension of the generator's input noise vector\n",
        "\n",
        "\n",
        "# Generator model\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(seq_length, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "\n",
        "# Discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_dim=seq_length))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# GAN model combining generator and discriminator\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(shape=(latent_dim,))\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = Model(gan_input, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        "\n",
        "\n",
        "# Generate sequences using the trained generator\n",
        "def generate_sequences(generator, num_sequences=64):\n",
        "    noise = np.random.randn(num_sequences, latent_dim)\n",
        "    generated_sequences = generator.predict(noise)\n",
        "    return (generated_sequences > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# Build and compile the models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train_gan(epochs=2000, batch_size=64):\n",
        "    for epoch in range(epochs):\n",
        "        # Generate real and fake samples\n",
        "        real_sequences = np.random.randint(0, 2, size=(batch_size, seq_length))\n",
        "        noise = np.random.randn(batch_size, latent_dim) #latent vector fron N(0,1)\n",
        "        generated_sequences = generator.predict(noise)\n",
        "\n",
        "        # Label real and fake samples\n",
        "        y_real = np.ones((batch_size, 1))\n",
        "        y_fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        # Train discriminator\n",
        "        d_loss_real = discriminator.train_on_batch(real_sequences, y_real)\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_sequences, y_fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Generate noise for training generator\n",
        "        noise = np.random.randn(batch_size, latent_dim)\n",
        "        y_gen = np.ones((batch_size, 1))\n",
        "\n",
        "        # Train generator\n",
        "        g_loss = gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch} | D loss: {d_loss[0]} | D accuracy: \"\n",
        "                  f\"{100 * d_loss[1]} | G loss: {g_loss}\")\n",
        "\n",
        "\n",
        "# Train the GAN\n",
        "train_gan(epochs=2000, batch_size=64)\n",
        "\n",
        "# Generate some sequences\n",
        "generated_sequences = generate_sequences(generator, num_sequences=10)\n",
        "print(generated_sequences)\n",
        "\n",
        "\n",
        "generated_sequences = generate_sequences(generator, num_sequences=137).tolist()\n",
        "for i in generated_sequences:\n",
        "    print(len(i), i)\n",
        "\n",
        "for i in range(1, len(generated_sequences)):\n",
        "    print(generated_sequences[i] == generated_sequences[i-1])\n",
        "\n",
        "file_name = 'fake_sequences_gan.csv'\n",
        "\n",
        "# Writing to CSV\n",
        "with open(file_name, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(generated_sequences)\n",
        "\n",
        "\n",
        "# Loading sequences\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = 'fake_sequences_handwritten.csv'\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "fake_sequences_handwritten = []\n",
        "\n",
        "# Read the CSV file and populate the fake_sequences_handwritten\n",
        "with open(csv_file_path, 'r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        for item in range(len(row)):\n",
        "            row[item] = int(row[item])\n",
        "        fake_sequences_handwritten.append(row)\n",
        "\n",
        "# Print the resulting list\n",
        "fake_sequences_handwritten.pop(0)\n",
        "lst = np.array(fake_sequences_handwritten).tolist()\n",
        "fake_sequences_handwritten = lst\n",
        "print(len(fake_sequences_handwritten))\n",
        "\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = 'fake_sequences_tricky.csv'\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "fake_sequences_tricky = []\n",
        "\n",
        "# Read the CSV file and populate the fake_sequences_handwritten\n",
        "with open(csv_file_path, 'r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        for item in range(len(row)):\n",
        "            row[item] = int(row[item])\n",
        "        fake_sequences_tricky.append(row)\n",
        "\n",
        "# Print the resulting list\n",
        "fake_sequences_tricky = np.array(fake_sequences_tricky).tolist()\n",
        "print(len(fake_sequences_tricky))\n",
        "\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = 'reals.csv'h\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "reals = [[random.randint(0, 1) for i in range(200)] for j in\n",
        "         range(4 * len(fake_sequences_handwritten))]\n",
        "\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = 'fake_sequences_dmom.csv'\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "fake_sequences_dmom = []\n",
        "\n",
        "# Read the CSV file and populate the fake_sequences_dmom\n",
        "with open(csv_file_path, 'r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        for item in range(len(row)):\n",
        "            row[item] = int(row[item])\n",
        "        fake_sequences_dmom.append(row)\n",
        "\n",
        "fake_sequences_dmom = np.array(fake_sequences_dmom).tolist()\n",
        "print(len(fake_sequences_dmom))\n",
        "\n",
        "\n",
        "# Take 80% of each type of sequence for training and leaving rest for testing\n",
        "train_ratio = 0.8\n",
        "\n",
        "reals_training, reals_testing = \\\n",
        "    train_test_split(reals, train_size=train_ratio, random_state=42)\n",
        "print('No. reals training: ' + str(len(reals_training)))\n",
        "print('No. reals testing: ' + str(len(reals_testing)) + '\\n')\n",
        "\n",
        "hand_fakes_training, hand_fakes_testing =\\\n",
        "    train_test_split(fake_sequences_handwritten,\n",
        "                     train_size=train_ratio, random_state=43)\n",
        "print('No. hand fakes training: ' + str(len(hand_fakes_training)))\n",
        "print('No. hand fakes testing: ' + str(len(hand_fakes_testing)) + '\\n')\n",
        "\n",
        "tricky_fakes_training, tricky_fakes_testing = \\\n",
        "    train_test_split(fake_sequences_tricky,\n",
        "                     train_size=train_ratio, random_state=44)\n",
        "print('No. tricky fakes training: ' + str(len(tricky_fakes_training)))\n",
        "print('No. tricky fakes testing: ' + str(len(tricky_fakes_testing)) + '\\n')\n",
        "\n",
        "dmom_fakes_training, dmom_fakes_testing = \\\n",
        "    train_test_split(fake_sequences_dmom,\n",
        "                     train_size=train_ratio, random_state=45)\n",
        "print('No. dmom fakes training: ' + str(len(dmom_fakes_training)))\n",
        "print('No. dmom fakes testing: ' + str(len(dmom_fakes_testing)) + '\\n')\n",
        "\n",
        "gan_fakes_training, gan_fakes_testing =\\\n",
        "    train_test_split(generated_sequences,\n",
        "                     train_size=train_ratio, random_state=46)\n",
        "print('No. GAN fakes training: ' + str(len(gan_fakes_training)))\n",
        "print('No. GAN fakes testing: ' + str(len(gan_fakes_testing)) + '\\n')\n",
        "\n",
        "\n",
        "# Training\n",
        "\n",
        "outside_sequences = np.array(hand_fakes_training + tricky_fakes_training +\n",
        "                             dmom_fakes_training + gan_fakes_training)\n",
        "reals = np.array(reals_training)\n",
        "\n",
        "# Combine real and outside sequences\n",
        "combined_sequences = np.vstack((reals, outside_sequences))\n",
        "labels = np.vstack((np.ones((reals.shape[0], 1)),\n",
        "                    np.zeros((outside_sequences.shape[0], 1))))\n",
        "\n",
        "# Shuffle the data\n",
        "indices = np.arange(combined_sequences.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "combined_sequences = combined_sequences[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Train the discriminator\n",
        "discriminator.fit(combined_sequences, labels, epochs=1000, batch_size=64)\n",
        "\n",
        "\n",
        "# Testing\n",
        "\n",
        "test_sequences = reals_testing + hand_fakes_testing + tricky_fakes_testing +\\\n",
        "                 dmom_fakes_testing + gan_fakes_testing\n",
        "\n",
        "test_labels = [1] * len(reals_testing) + [0] * len(hand_fakes_testing) + [0] *\\\n",
        "              len(tricky_fakes_testing) + [0] * len(dmom_fakes_testing) + [0] *\\\n",
        "              len(gan_fakes_testing)\n",
        "\n",
        "# Loop through sequences and print results\n",
        "# Loop through sequences and print results\n",
        "for i in range(len(test_sequences)):\n",
        "    sequence = np.array(test_sequences[i])  # Convert to numpy array\n",
        "    true_label = test_labels[i]\n",
        "\n",
        "    # Reshape the sequence if necessary (depending on your model's input shape)\n",
        "    sequence = np.reshape(sequence, (1, -1))\n",
        "\n",
        "    # Predict the label for the sequence\n",
        "    prediction = discriminator.predict(sequence)\n",
        "\n",
        "    # Convert the prediction to 0 or 1 based on a threshold (e.g., 0.5)\n",
        "    predicted_label = 1 if prediction > 0.5 else 0\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Sequence {i+1}: True Label={true_label},\"\n",
        "          f\" Predicted Label={predicted_label}\")\n",
        "\n",
        "\n",
        "# Evaluate the discriminator for overall accuracy\n",
        "test_loss, test_accuracy = discriminator.evaluate(test_sequences, test_labels)\n",
        "\n",
        "print(f\"\\nOverall Test Loss: {test_loss}\")\n",
        "print(f\"Overall Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "\n",
        "# DMOM Testing\n",
        "test_sequences = dmom_fakes_testing\n",
        "\n",
        "test_labels = [0] * (len(dmom_fakes_testing))\n",
        "\n",
        "# Loop through sequences and print results\n",
        "for i in range(len(test_sequences)):\n",
        "    sequence = test_sequences[i]\n",
        "    true_label = test_labels[i]\n",
        "\n",
        "    # Reshape the sequence if necessary (depending on your model's input shape)\n",
        "    sequence = np.reshape(sequence, (1, -1))\n",
        "\n",
        "    # Predict the label for the sequence\n",
        "    prediction = discriminator.predict(sequence)\n",
        "\n",
        "    # Convert the prediction to 0 or 1 based on a threshold (e.g., 0.5)\n",
        "    predicted_label = 1 if prediction > 0.5 else 0\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Sequence {i+1}: True Label={true_label},\"\n",
        "          f\" Predicted Label={predicted_label}\")\n",
        "\n",
        "# Evaluate the discriminator for overall accuracy\n",
        "test_loss, test_accuracy = discriminator.evaluate(test_sequences, test_labels)\n",
        "\n",
        "print(f\"\\nOverall Test Loss: {test_loss}\")\n",
        "print(f\"Overall Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "ohR_aJ_dNzYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DMOM"
      ],
      "metadata": {
        "id": "ODYefnieizAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 128\n",
        "print(s)\n",
        "b = 2\n",
        "print(b)\n",
        "N = 200\n",
        "print(N)\n",
        "\n",
        "# Canonical model\n",
        "# test for s = 2\n",
        "p = []\n",
        "for i in range(2):\n",
        "    p_r = np.random.uniform(0, 1, 2)\n",
        "    p_r /= p_r.sum()\n",
        "    p.append(p_r)\n",
        "p = np.array(p)\n",
        "q0_real0 = [[0.5, 0.5], [0.5, 0.5]]\n",
        "q1_real0 = [[0.5, 0.5], [0.5, 0.5]]\n",
        "q = np.array([q0_real0, q1_real0])\n",
        "mu = np.array([[0.25, 0.25], [0.25, 0.25]])\n",
        "\n",
        "num_states = b * 2\n",
        "states = []  # List of the states in the joint MC\n",
        "for i in range(2):\n",
        "    for j in range(b):\n",
        "        states.append((i, j))\n",
        "\n",
        "states_dict = {}  # Assigns a temp value to each state in the joint MC\n",
        "for i in range(len(states)):\n",
        "    states_dict[i] = states[i]\n",
        "\n",
        "# Creating transition matrix\n",
        "lst = []\n",
        "for state in states_dict:\n",
        "    row = []\n",
        "    for state2 in states_dict:\n",
        "        initial = states_dict[state]\n",
        "        final = states_dict[state2]\n",
        "        p_val = p[initial[0], final[0]]\n",
        "        q_val = q[final[0]][initial[1], final[1]]\n",
        "        row.append(p_val * q_val)\n",
        "    lst.append(row)\n",
        "\n",
        "joint_transition_mat = np.array(lst)\n",
        "print('Joint transition matrix: \\n', joint_transition_mat)\n",
        "\n",
        "\n",
        "# Initialize the sequence by randomly choosing the initial state based off mu\n",
        "seq = []\n",
        "lst = [x for x in range(b * 2)]\n",
        "weights = []\n",
        "for i in lst:\n",
        "    weights.append(mu[states_dict[i][0], states_dict[i][1]])\n",
        "seq.append(np.random.choice(lst, 1, p=weights)[0])\n",
        "\n",
        "# Randomly choosing the next state based off the probabilities in the joint\n",
        "# transition matrix\n",
        "while len(seq) != 201:\n",
        "    initial = seq[-1]\n",
        "    seq.append(np.random.choice(lst, 1,  p=[item for item in\n",
        "                                            joint_transition_mat[initial]])[0])\n",
        "\n",
        "# Removing the hidden state and keeping only the observations\n",
        "final_seq = []\n",
        "for item in seq:\n",
        "    final_seq.append(states_dict[item][1])\n",
        "final_seq.pop(0)\n",
        "\n",
        "print(final_seq, len(final_seq))\n",
        "one_count = 0\n",
        "print(final_seq.count(0), final_seq.count(1))\n",
        "\n",
        "\n",
        "def read_csv_as_array(name):\n",
        "    data_frame = pd.read_csv(name, header=None)\n",
        "    data_array = data_frame.values\n",
        "    return data_array\n",
        "\n",
        "\n",
        "hand_fakes = read_csv_as_array('fake_sequences_handwritten.csv').tolist()\n",
        "hand_fakes.pop(0)\n",
        "print('No. hand fakes: ' + str(len(hand_fakes)))\n",
        "\n",
        "gan_fakes = read_csv_as_array('fake_sequences_gan.csv').tolist()\n",
        "print('No. gan fakes: ' + str(len(gan_fakes)))\n",
        "\n",
        "tricky_fakes = read_csv_as_array('fake_sequences_tricky.csv').tolist()\n",
        "print('No. tricky fakes: ' + str(len(tricky_fakes)))\n",
        "\n",
        "random_arrays = [final_seq]\n",
        "reals = [[random.randint(0, 1) for i in range(200)] for j in\n",
        "         range(4 * (len(hand_fakes)) - 1)]\n",
        "random_arrays += reals\n",
        "print('No. reals: ' + str(len(random_arrays)) + '\\n')\n",
        "\n",
        "# Test-train split\n",
        "train_ratio = 0.8\n",
        "\n",
        "reals_training, reals_testing = \\\n",
        "    train_test_split(reals, train_size=train_ratio, random_state=42)\n",
        "print('No. reals training: ' + str(len(reals_training)))\n",
        "print('No. reals testing: ' + str(len(reals_testing)) + '\\n')\n",
        "\n",
        "hand_fakes_training, hand_fakes_testing = \\\n",
        "    train_test_split(hand_fakes,\n",
        "                     train_size=train_ratio, random_state=43)\n",
        "print('No. hand fakes training: ' + str(len(hand_fakes_training)))\n",
        "print('No. hand fakes testing: ' + str(len(hand_fakes_testing)) + '\\n')\n",
        "\n",
        "tricky_fakes_training, tricky_fakes_testing = \\\n",
        "    train_test_split(tricky_fakes,\n",
        "                     train_size=train_ratio, random_state=44)\n",
        "print('No. tricky fakes training: ' + str(len(tricky_fakes_training)))\n",
        "print('No. tricky fakes testing: ' + str(len(tricky_fakes_testing)) + '\\n')\n",
        "\n",
        "gan_fakes_training, gan_fakes_testing = \\\n",
        "    train_test_split(gan_fakes,\n",
        "                     train_size=train_ratio, random_state=46)\n",
        "print('No. GAN fakes training: ' + str(len(gan_fakes_training)))\n",
        "print('No. GAN fakes testing: ' + str(len(gan_fakes_testing)) + '\\n')\n",
        "\n",
        "\n",
        "# Real sequences\n",
        "def ForwardpiZero(mu_matrix):\n",
        "    pi_zero = mu_matrix\n",
        "    return pi_zero\n",
        "\n",
        "\n",
        "def Forwardpi(p, q, mu, N, Y):\n",
        "    pi = np.zeros((s,N)).reshape(s,N) # Initialization\n",
        "    rho = np.zeros((s, N))\n",
        "    a = np.zeros(N)\n",
        "    for x in range(s):\n",
        "        rho[x, 0] = np.sum([np.sum(mu[x_0, :]*p[x_0, x]*q[x, :, Y[0]]) for x_0\n",
        "                            in range(s)])\n",
        "    a[0] = np.sum(rho[:, 0])\n",
        "    pi[:, 0] = rho[:, 0]/a[0]\n",
        "\n",
        "    for n in range(1, N):\n",
        "        rho[:, n] =\\\n",
        "            q[:, Y[n-1],\n",
        "                Y[n]]*(np.sum(pi[:, n - 1].reshape(s, 1)*p[:, :], axis=0))\n",
        "        a[n] = np.sum(rho[:, n])\n",
        "        pi[:, n] = rho[:, n]/a[n]\n",
        "    return pi, a, rho\n",
        "\n",
        "\n",
        "def Backwardchi(p, q, a, N, Y):\n",
        "    chi = np.zeros((s, N-1)).reshape(s, N-1)\n",
        "    chi[:, N - 2] = q[:, Y[N-2], Y[N-1]]\n",
        "\n",
        "    for n in range(N-3, -1, -1):\n",
        "        chi[:, n] = ((q[:, Y[n],Y[n+1]].reshape(s,1)/a[n+1]) *\n",
        "                     (np.sum((chi[:, n+1].reshape(1, s) *\n",
        "                              p[:, :]), axis=1).reshape(s, 1))).reshape(s, )\n",
        "\n",
        "    return chi\n",
        "\n",
        "\n",
        "def BackwardchiZero(chi, p, q, a, Y):\n",
        "    chi_zero = np.zeros((s, b)).reshape(s,b)\n",
        "    for y in range(b):\n",
        "        chi_zero[:, y] = ((q[:, y, Y[0]].reshape(s, 1)/a[0]) *\n",
        "                          (np.sum((chi[:, 0].reshape(1, s)*p[:, :]),\n",
        "                                  axis=1).reshape(s, 1))).reshape(s, )\n",
        "    return chi_zero\n",
        "\n",
        "\n",
        "def MQ(p, pi_zero, pi, chi_zero, chi, N, Y):\n",
        "    q_n = np.zeros((s, b, b))\n",
        "    for y in range(b):\n",
        "        q_n[:, y, Y[0]] = (chi_zero[:, y].reshape(s, 1) *\n",
        "                           (np.sum(p[:, :] * pi_zero[:, y].reshape(s, 1),\n",
        "                                   axis=0).reshape(s, 1))).reshape(s, )\n",
        "    for n in range(N - 1):\n",
        "        q_n[:, Y[n], Y[n + 1]] = (q_n[:, Y[n], Y[n + 1]].reshape(s, 1) + chi[:, n].reshape(s, 1) * (np.sum(p[:, :] * pi[:, n].reshape(s, 1), axis = 0).reshape(s, 1))).reshape(s, )\n",
        "    r_sum = q_n.sum(axis=2)\n",
        "    for r in range(b):\n",
        "        for x in range(s):\n",
        "            if r_sum[x, r] != 0:\n",
        "                q_n[x, r, :] = q_n[x, r, :] / r_sum[x, r]\n",
        "\n",
        "    return q_n\n",
        "\n",
        "\n",
        "def MMu(p, mu, chi_zero):\n",
        "    m_new = np.zeros((s, b))\n",
        "    for y in range(b):\n",
        "        m_new[:, y] = (mu[:, y].reshape(s, 1) *\n",
        "                       (p @ chi_zero[:, y].reshape(s, 1))).reshape(s, )\n",
        "        total = sum(list(m_new.sum(axis=1)))\n",
        "    for i in range(s):\n",
        "        for j in range(b):\n",
        "            if total != 0:\n",
        "                m_new[i, j] = m_new[i, j] / total\n",
        "\n",
        "    return m_new\n",
        "\n",
        "\n",
        "def MP(p, pi_zero, pi, chi_zero, chi):\n",
        "    old = p\n",
        "    new = old * (pi_zero @ chi_zero.T + pi[:, :-1] @ chi.T)\n",
        "    r_sum = new.sum(axis=1)\n",
        "    new = new / r_sum[:, np.newaxis]\n",
        "\n",
        "    return new\n",
        "\n",
        "\n",
        "def InitialP(k):\n",
        "    p_v = []\n",
        "    for i in range(k):\n",
        "        r = np.random.uniform(0, 1, k)\n",
        "        r /= r.sum()\n",
        "        p_v.append(r)\n",
        "    p = np.array(p_v)\n",
        "    return p\n",
        "\n",
        "\n",
        "def InitialQ(N, Y, s: int):\n",
        "    q = np.zeros((s, b, b))\n",
        "    for x in range(s):\n",
        "        for n in range(N-1):\n",
        "            q[x, Y[n],Y[n+1]] += 1\n",
        "        r_sum = q.sum(axis=2)\n",
        "        for r in range(b):\n",
        "            for c in range(b):\n",
        "                if r_sum[x, r] > 0 and q[x, r, c] > 0:\n",
        "                    q[x, r, c] = q[x, r, c] * np.random.uniform(0, 1, 1)\n",
        "        row_sum_new = q.sum(axis=2)\n",
        "        for i in range(b):\n",
        "            for j in range(b):\n",
        "                if row_sum_new[x, i] != 0:\n",
        "                    q[x, i, j] = q[x, i, j]/row_sum_new[x, i]\n",
        "    return q\n",
        "\n",
        "\n",
        "def Initialmu(q, Y):\n",
        "    mu = np.random.rand(s, b)\n",
        "    for x in range(s):\n",
        "        for r in range(b):\n",
        "            if q[x, r, Y[0]] > 0:\n",
        "                continue\n",
        "            else:\n",
        "                mu[x, r] = 0\n",
        "\n",
        "    sum = mu.sum()\n",
        "    mu = mu/sum\n",
        "    return mu\n",
        "\n",
        "\n",
        "def Q_ref(Y, N):\n",
        "    q_bar = np.zeros((b, b))\n",
        "    for i in range(1, N):\n",
        "        q_bar[Y[i-1], Y[i]] += 1\n",
        "    r_sum = list(q_bar.sum(axis=1))\n",
        "    for r in range(b):\n",
        "        if r_sum[r] != 0:\n",
        "            q_bar[r] = q_bar[r] / r_sum[r]\n",
        "    return q_bar\n",
        "\n",
        "\n",
        "def Predictorlist(p, q, mu, q_bar, Y, N):\n",
        "    predictor_dist = np.zeros((N, s))\n",
        "    temp = np.zeros(s)\n",
        "    for x in range(s):\n",
        "        temp_ = np.array([(q[x, z, Y[0]]/q_bar[z, Y[0]] if q_bar[z, Y[0]] != 0\n",
        "                           else 0) for z in range(b)])\n",
        "        temp[x] = np.sum([np.sum(mu[x_0, :]*p[x_0, x]*temp_)\n",
        "                          for x_0 in range(s)])\n",
        "\n",
        "    for i in range(N):\n",
        "        for x in range(s):\n",
        "            if i == 0:\n",
        "                predictor_dist[0, x] = temp @ p[:, x]\n",
        "            else:\n",
        "                temp_ = q[:, Y[i - 1], Y[i]]/q_bar[Y[i - 1], Y[i]] if q_bar[Y[i - 1], Y[i]] != 0 else 0\n",
        "                predictor_dist[i, x] = np.sum(temp_ * p[:, x] * predictor_dist[i - 1, :])\n",
        "\n",
        "    return predictor_dist\n",
        "\n",
        "\n",
        "def BayesFactor(predictor_rho, q, N, Y):\n",
        "    B = np.zeros(N)\n",
        "    for i in range(N):\n",
        "        B[i] = np.sum(predictor_rho[i, :])\n",
        "    B[-1] = np.sum([((predictor_rho[N-2, x] * q[x, Y[-2], Y[-1]]/q_bar[Y[-2], Y[-1]]) if q_bar[Y[-2], Y[-1]] != 0 else 0) for x in range(s)])\n",
        "    return B\n",
        "\n",
        "\n",
        "def Stop(p_new, mu_new, q_new, p, mu, q, k):\n",
        "    p_new = p_new\n",
        "    p = p\n",
        "    difp = np.subtract(p_new, p)\n",
        "    sdifp = np.nansum(np.abs(difp))\n",
        "    q = q\n",
        "    q_new = q_new\n",
        "    mu_new = mu_new\n",
        "    mu = mu\n",
        "    difmu = np.subtract(mu_new, mu)\n",
        "    sdifmu = np.nansum(np.abs(difmu))\n",
        "\n",
        "    np.set_printoptions(suppress=True)\n",
        "\n",
        "    if sdifp < 5e-3 and sdifmu < 5e-3:\n",
        "        return 'stop'\n",
        "\n",
        "    else:\n",
        "        return 'continue'\n",
        "\n",
        "\n",
        "def RunModel(p, q, mu, N, Y, stoplimit = 1000):\n",
        "    K = stoplimit\n",
        "    for k in range(K):\n",
        "        pi_zero = ForwardpiZero(mu)\n",
        "        pi, a, rho = Forwardpi(p,q,mu,N,Y)\n",
        "        chi = Backwardchi(p,q, a,N,Y)\n",
        "        chi_zero = BackwardchiZero(chi, p, q, a, Y)\n",
        "        q_new = MQ(p, pi_zero, pi, chi_zero, chi, N, Y)\n",
        "        mu_new = MMu(p, mu, chi_zero)\n",
        "        p_new = MP(p, pi_zero, pi, chi_zero, chi)\n",
        "        stopcondition = Stop(p_new,mu_new,q_new, p, mu, q, k)\n",
        "\n",
        "        if stopcondition == \"stop\":\n",
        "            # print(\"Stopping criteria reached at k = \", k)\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            p = p_new\n",
        "            mu = mu_new\n",
        "            q = q_new\n",
        "\n",
        "        return p, q, mu\n",
        "\n",
        "\n",
        "pqmus_real = {}\n",
        "generated_sequences = []\n",
        "\n",
        "for k in range(len(reals_training)):\n",
        "\n",
        "    Y = random_arrays[k]\n",
        "\n",
        "    q_bar = Q_ref(Y, N)\n",
        "\n",
        "    num_particles = 10\n",
        "    model_initializations = {}\n",
        "    model_results = {}\n",
        "    comparison_dict = {}\n",
        "    for i in range(1, num_particles + 1):\n",
        "        model_name = f\"model{i}\"\n",
        "\n",
        "        model_initializations[model_name] = {}\n",
        "        model_initializations[model_name]['P'] = InitialP(s)\n",
        "        model_initializations[model_name]['Q'] = InitialQ(N, Y, s)\n",
        "        model_initializations[model_name]['mu'] = Initialmu(model_initializations[model_name]['Q'], Y)\n",
        "\n",
        "        FinalP, FinalQ, Finalmu = RunModel(model_initializations[model_name]['P'], model_initializations[model_name]['Q'], model_initializations[model_name]['mu'], N, Y)\n",
        "        Predictor = Predictorlist(FinalP, FinalQ, Finalmu, q_bar, Y, N)\n",
        "        comparison_dict[model_name] = np.log(BayesFactor(Predictor, FinalQ, N, Y)[-1]) # Take log of the Bayes Factor, since it expands exponentially.\n",
        "\n",
        "        model_results[model_name] = {}\n",
        "        model_results[model_name]['P'] = FinalP\n",
        "        model_results[model_name]['Q'] = FinalQ\n",
        "        model_results[model_name]['mu'] = Finalmu\n",
        "\n",
        "    pqmus_real[k] = {'p':FinalP, 'q':FinalQ, 'mu':Finalmu}\n",
        "\n",
        "    if k < len(hand_fakes):\n",
        "        num_states = b * s\n",
        "        states = [] # List of the states in the joint MC\n",
        "        for i in range(s):\n",
        "            for j in range(b):\n",
        "                states.append((i, j))\n",
        "\n",
        "        states_dict = {} # Assigns a temp value to each state in the joint MC\n",
        "        for i in range(len(states)):\n",
        "            states_dict[i] = states[i]\n",
        "\n",
        "        # Creating transition matrix\n",
        "        lst = []\n",
        "        for state in states_dict:\n",
        "            row = []\n",
        "            for state2 in states_dict:\n",
        "                initial = states_dict[state]\n",
        "                final = states_dict[state2]\n",
        "                p_val = FinalP[initial[0], final[0]]\n",
        "                q_val = FinalQ[final[0]][initial[1], final[1]]\n",
        "                row.append(p_val * q_val)\n",
        "            lst.append(row)\n",
        "\n",
        "        joint_transition_mat = np.array(lst)\n",
        "        # print('Joint transition matrix: \\n', joint_transition_mat)\n",
        "\n",
        "        # Initialize the sequence by randomly choosing the initial state\n",
        "        # based off mu\n",
        "        seq = []\n",
        "        lst = [x for x in range(b * s)]\n",
        "        weights = []\n",
        "        for i in lst:\n",
        "            weights.append(Finalmu[states_dict[i][0], states_dict[i][1]])\n",
        "        seq.append(np.random.choice(lst, 1, p = weights)[0])\n",
        "\n",
        "        # Randomly choosing the next state based off the probabilities in the\n",
        "        # joint transition matrix\n",
        "        while len(seq) != 201:\n",
        "            initial = seq[-1]\n",
        "            seq.append(np.random.choice(lst, 1, p = [item for item in joint_transition_mat[initial]])[0])\n",
        "\n",
        "        # Removing the hidden state and keeping only the observations\n",
        "        final_seq = []\n",
        "        for item in seq:\n",
        "            final_seq.append(states_dict[item][1])\n",
        "        final_seq.pop(0)\n",
        "\n",
        "        generated_sequences.append(final_seq)\n",
        "\n",
        "    print(f'Num reals left: {len(reals_training) - k}')\n",
        "\n",
        "\n",
        "# Dimension increase\n",
        "s = s * 2\n",
        "\n",
        "# Bayes Factor Reals Dimension increase\n",
        "pqmus_real_higher_dim = {}\n",
        "for sequence in pqmus_real:\n",
        "    p_old = pqmus_real[sequence]['p']\n",
        "    q_old = pqmus_real[sequence]['q']\n",
        "    mu_old = pqmus_real[sequence]['mu']\n",
        "\n",
        "    # Increase dimension of p_old\n",
        "    array = p_old.tolist()\n",
        "    vals = []\n",
        "    for i in array:\n",
        "        for j in i:\n",
        "            vals.append(j)\n",
        "    for i in range(s):\n",
        "        if i % 2 == 0:\n",
        "            for j in range(s):\n",
        "                if j % 2 == 1:\n",
        "                    array[i].insert(j, 0)\n",
        "        if i % 2 == 1:\n",
        "            row = []\n",
        "            for k in range(s):\n",
        "                row.append(np.random.uniform(0, st.mean(vals)))\n",
        "            row_sum = sum(row)\n",
        "            row = np.divide(row, row_sum)\n",
        "            array.insert(i, row)\n",
        "\n",
        "    p_new = 0.5 * np.array(array) + (1 / (2 * s)) * np.ones_like(s)\n",
        "\n",
        "    # Increase dimension of q_old\n",
        "    q_new_matrices = {}\n",
        "    for i in range(s):\n",
        "        if i % 2 == 0:\n",
        "            q_new_matrices[i] = q_old[int(i / 2)]\n",
        "\n",
        "        elif i % 2 == 1:\n",
        "            matrix = []\n",
        "            for j in range(b):\n",
        "                row = [np.random.uniform(0, st.mean(q_old[int((i-1) / 2)].tolist()[j])) for _ in range(b)]\n",
        "                row = np.divide(row, sum(row))\n",
        "                matrix.append(row)\n",
        "            q_new_matrices[i] = np.array(matrix)\n",
        "    array = []\n",
        "    for i in range(s):\n",
        "        array.append(q_new_matrices[i])\n",
        "    q_new = np.array(array)\n",
        "\n",
        "    # Showing that the sum of each row in each q in q_new is 1\n",
        "    q_sums = []\n",
        "    for i in range(s):\n",
        "        q_sums.append([f'q_new{i}'] + [sum(row) for row in q_new_matrices[i].tolist()])\n",
        "\n",
        "    # Increase dimension of mu_old\n",
        "    array = mu_old.tolist()\n",
        "    vals = []\n",
        "    for i in array:\n",
        "        for j in i:\n",
        "            vals.append(j)\n",
        "    for i in range(s):\n",
        "        if i % 2 == 1:\n",
        "            row = []\n",
        "            for j in range(b):\n",
        "                row.append(np.random.uniform(0, st.mean(vals)))\n",
        "            array.insert(i, row)\n",
        "    mu_new = np.array(array) * 1 / (np.array(array).sum())\n",
        "\n",
        "    # Showing that the matrix sum of mu_new is 1\n",
        "\n",
        "    pqmus_real_higher_dim[sequence] = {'p': p_new, 'q': q_new, 'mu': mu_new}\n",
        "\n",
        "\n",
        "# Generated sequences\n",
        "generated_training, generated_testing = \\\n",
        "    train_test_split(generated_sequences, train_size=train_ratio,\n",
        "                     random_state=50)\n",
        "print('No. dmom training: ' + str(len(generated_sequences)))\n",
        "print('No. dmom testing: ' + str(len(generated_testing)) + '\\n')\n",
        "\n",
        "\n",
        "# Fake sequences\n",
        "random_arrays_fakes = hand_fakes_training + tricky_fakes_training +\\\n",
        "                      gan_fakes_training + generated_training\n",
        "\n",
        "\n",
        "# Fake sequence p, q, mu\n",
        "pqmus_fake = {}\n",
        "\n",
        "for k in range(len(random_arrays_fakes)):\n",
        "    Y = random_arrays_fakes[k]\n",
        "\n",
        "    print(Y, len(Y))\n",
        "    q_bar = Q_ref(Y, N)\n",
        "\n",
        "    num_particles = 10\n",
        "    model_initializations = {}\n",
        "    model_results = {}\n",
        "    comparison_dict = {}\n",
        "    for i in range(1, num_particles + 1):\n",
        "        model_name = f\"model{i}\"\n",
        "\n",
        "        model_initializations[model_name] = {}\n",
        "        model_initializations[model_name]['P'] = InitialP(s)\n",
        "        model_initializations[model_name]['Q'] = InitialQ(N, Y, s)\n",
        "        model_initializations[model_name]['mu'] = Initialmu(model_initializations[model_name]['Q'], Y)\n",
        "\n",
        "        FinalP, FinalQ, Finalmu = RunModel(model_initializations[model_name]['P'], model_initializations[model_name]['Q'], model_initializations[model_name]['mu'], N, Y)\n",
        "        Predictor = Predictorlist(FinalP, FinalQ, Finalmu, q_bar, Y, N)\n",
        "        comparison_dict[model_name] = np.log(BayesFactor(Predictor, FinalQ, N, Y)[-1]) # Take log of the Bayes Factor, since it expands exponentially.\n",
        "\n",
        "        model_results[model_name] = {}\n",
        "        model_results[model_name]['P'] = FinalP\n",
        "        model_results[model_name]['Q'] = FinalQ\n",
        "        model_results[model_name]['mu'] = Finalmu\n",
        "\n",
        "    pqmus_fake[k] = {'p':FinalP, 'q':FinalQ, 'mu':Finalmu}\n",
        "\n",
        "    print(f'Num fakes left: {len(random_arrays_fakes) - k}')\n",
        "\n",
        "\n",
        "# Testing reals\n",
        "highest_bayes = []\n",
        "theoretical = [f'R' for i in range(len(reals_testing))]\n",
        "\n",
        "count = 0\n",
        "for j in range(len(reals_testing)):\n",
        "    bayes_factors = {}\n",
        "    sequence = random_arrays[j]\n",
        "    q_bar = np.array([[.5, .5], [.5, .5]])\n",
        "    for i in pqmus_real_higher_dim:\n",
        "        p = pqmus_real_higher_dim[i]['p']\n",
        "        q = pqmus_real_higher_dim[i]['q']\n",
        "        mu = pqmus_real_higher_dim[i]['mu']\n",
        "        lst1 = BayesFactor(Predictorlist(p, q, mu, q_bar, sequence, N), q, N, sequence)\n",
        "        bayes_factors['Real' + str(i)] = lst1[-1]\n",
        "    for i in pqmus_fake:\n",
        "        p = pqmus_fake[i]['p']\n",
        "        q = pqmus_fake[i]['q']\n",
        "        mu = pqmus_fake[i]['mu']\n",
        "        lst3 = BayesFactor(Predictorlist(p, q, mu, q_bar, sequence, N), q, N, sequence)\n",
        "        bayes_factors['Fake' + str(i)] = lst3[-1]\n",
        "    print(max(bayes_factors, key=bayes_factors.get)[:1])\n",
        "    highest_bayes.append(max(bayes_factors, key=bayes_factors.get)[:1])\n",
        "\n",
        "print(theoretical)\n",
        "print(highest_bayes)\n",
        "print(f\"Correct: {highest_bayes.count('R') / len(reals_testing) * 100}%\")\n",
        "\n",
        "\n",
        "# Testing fakes\n",
        "fakes_testing = hand_fakes_testing + gan_fakes_testing + tricky_fakes_testing + generated_testing\n",
        "\n",
        "highest_bayes = []\n",
        "theoretical = [f'F' for i in range(len(fakes_testing))]\n",
        "\n",
        "count = 0\n",
        "for j in range(len(fakes_testing)):\n",
        "    bayes_factors = {}\n",
        "    sequence = random_arrays[j]\n",
        "    q_bar = np.array([[.5, .5], [.5, .5]])\n",
        "    for i in pqmus_real_higher_dim:\n",
        "        p = pqmus_real_higher_dim[i]['p']\n",
        "        q = pqmus_real_higher_dim[i]['q']\n",
        "        mu = pqmus_real_higher_dim[i]['mu']\n",
        "        lst1 = BayesFactor(Predictorlist(p, q, mu, q_bar, sequence, N), q, N, sequence)\n",
        "        bayes_factors['Real' + str(i)] = lst1[-1]\n",
        "    for i in pqmus_fake:\n",
        "        p = pqmus_fake[i]['p']\n",
        "        q = pqmus_fake[i]['q']\n",
        "        mu = pqmus_fake[i]['mu']\n",
        "        lst3 = BayesFactor(Predictorlist(p, q, mu, q_bar, sequence, N), q, N, sequence)\n",
        "        bayes_factors['Fake' + str(i)] = lst3[-1]\n",
        "    print(max(bayes_factors, key=bayes_factors.get)[:1])\n",
        "    highest_bayes.append(max(bayes_factors, key=bayes_factors.get)[:1])\n",
        "\n",
        "print(theoretical)\n",
        "print(highest_bayes)\n",
        "print(f'All correctly identified? {theoretical == highest_bayes}')\n",
        "print(f\"Correct: {highest_bayes.count('F') / len(fakes_testing) * 100}%\")\n",
        "\n",
        "print('Done')\n"
      ],
      "metadata": {
        "id": "FQwkHHitORhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "T2zho2hhhb1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_sequences = 200\n",
        "seq_length = 200\n",
        "\n",
        "# Generate 10 random binary sequences of length 200 for reals and fakes (for now using different random functions)\n",
        "fakes = []\n",
        "reals = []\n",
        "for i in range(num_sequences):\n",
        "    fake = [random.choice([0, 1]) for j in range(seq_length)]\n",
        "    fakes.append(fake)\n",
        "    real = [random.randint(0, 1) for j in range(seq_length)]\n",
        "    reals.append(real)\n",
        "\n",
        "fake_labels = [0]*num_sequences\n",
        "real_labels = [1]*num_sequences\n",
        "\n",
        "combined_sequences = reals + fakes\n",
        "combined_labels = real_labels + fake_labels\n",
        "\n",
        "\n",
        "#for sequence, label in zip(combined_sequences, combined_labels):\n",
        "#    print(f\"{sequence}, Label: {label}\")"
      ],
      "metadata": {
        "id": "gqS9Qz0FiCV5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics._plot.confusion_matrix import confusion_matrix\n",
        "X_train, X_test, y_train, y_test = train_test_split(combined_sequences, combined_labels, test_size=0.2, random_state=1)\n",
        "#random_state = 0 ensures same split every time\n",
        "#test_size = 0.2 is 80:20 split\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(confusion_matrix(y_test, svm_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-4xugpRrG3f",
        "outputId": "7dfd2c5f-8466-428b-df6e-db7ccf8bdede"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.60      0.62        47\n",
            "           1       0.47      0.52      0.49        33\n",
            "\n",
            "    accuracy                           0.56        80\n",
            "   macro avg       0.55      0.56      0.55        80\n",
            "weighted avg       0.57      0.56      0.56        80\n",
            "\n",
            "[[28 19]\n",
            " [16 17]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_class_svm_model = OneClassSVM(kernel='rbf', nu=0.1)  # You can adjust the hyperparameters as needed\n",
        "one_class_svm_model.fit(X_train)\n",
        "\n",
        "ocsvm_predictions = one_class_svm_model.predict(X_test)\n",
        "\n",
        "#modify 0s to -1's in y_test\n",
        "y_test_mod = np.where(y_test == 0, -1, y_test)\n",
        "\n",
        "roc_auc = roc_auc_score(y_test_mod, ocsvm_predictions)\n",
        "pr_auc = average_precision_score(y_test_mod, ocsvm_predictions)\n",
        "\n",
        "print(\"One-Class SVM Evexaluation Metrics:\")\n",
        "print(\"AUC-ROC: {:.2f}\".format(roc_auc))\n",
        "print(\"AUC-PR: {:.2f}\".format(pr_auc))\n"
      ],
      "metadata": {
        "id": "3ZKjD6DfuJgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbf665d-0ff3-4254-8c1e-9a36240eee92"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Class SVM Evexaluation Metrics:\n",
            "AUC-ROC: 0.50\n",
            "AUC-PR: 0.41\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.00      0.00      0.00       0.0\n",
            "           0       0.00      0.00      0.00      47.0\n",
            "           1       0.00      0.00      0.00      33.0\n",
            "\n",
            "    accuracy                           0.00      80.0\n",
            "   macro avg       0.00      0.00      0.00      80.0\n",
            "weighted avg       0.00      0.00      0.00      80.0\n",
            "\n",
            "[[ 0  0  0]\n",
            " [47  0  0]\n",
            " [33  0  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}